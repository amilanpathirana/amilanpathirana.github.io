<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Scary Ghost]]></title><description><![CDATA[Randoms]]></description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>Scary Ghost</title><link>http://localhost:2368/</link></image><generator>Ghost 4.1</generator><lastBuildDate>Sat, 10 Apr 2021 03:41:08 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Backpropagation]]></title><description><![CDATA[<p>Heart of training a neural network is the backpropagation algorithm that calculates the gradients of weights and biases with respect to the error.</p><p>Goal of calculating these gradients is to use gradient decent optimization method to find new weights and biases such that it minimizes the error.</p><p>Following video shows</p>]]></description><link>http://localhost:2368/backpropagation/</link><guid isPermaLink="false">60694328107808500c7cfdc2</guid><dc:creator><![CDATA[Amila Pathirana]]></dc:creator><pubDate>Thu, 07 May 2020 04:43:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1545987796-200677ee1011?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fG5ldXJhbCUyMG5ldHdvcmt8ZW58MHx8fHwxNjE3NTExMzQ2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1545987796-200677ee1011?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDJ8fG5ldXJhbCUyMG5ldHdvcmt8ZW58MHx8fHwxNjE3NTExMzQ2&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Backpropagation"><p>Heart of training a neural network is the backpropagation algorithm that calculates the gradients of weights and biases with respect to the error.</p><p>Goal of calculating these gradients is to use gradient decent optimization method to find new weights and biases such that it minimizes the error.</p><p>Following video shows how the backpropagation algorithm works for simplest of neural network. As the neural network gets deep and wide, core principles remains unchanged.</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/8A93E1xVvHI?start=1&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure>]]></content:encoded></item></channel></rss>